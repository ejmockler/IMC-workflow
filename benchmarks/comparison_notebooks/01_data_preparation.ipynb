{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Preparation for Benchmarking\n",
    "\n",
    "**Goal**: Download, validate, and format public IMC datasets for comparison between Steinbock and our pipeline.\n",
    "\n",
    "## Datasets\n",
    "- **Bodenmiller Example**: Small tutorial dataset from Zenodo (5949116)\n",
    "- **High-res Kidney**: Tissue-matched high-resolution IMC (Zenodo 10.5281/zenodo.17077712)\n",
    "\n",
    "## Tasks\n",
    "1. Download datasets from Zenodo\n",
    "2. Verify data integrity (file formats, channel counts)\n",
    "3. Create metadata CSV (ROI names, conditions, timepoints)\n",
    "4. Generate summary statistics\n",
    "5. Prepare for both pipelines (Steinbock .txt format, our pipeline format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import hashlib\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from src.utils.helpers import ROIMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "benchmark_dir = project_root / 'benchmarks'\n",
    "data_dir = benchmark_dir / 'data'\n",
    "\n",
    "# Dataset selection\n",
    "DATASET_NAME = 'bodenmiller_example'  # or 'highres_kidney_2025'\n",
    "dataset_path = data_dir / DATASET_NAME\n",
    "\n",
    "print(f\"Benchmark directory: {benchmark_dir}\")\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Dataset exists: {dataset_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "**Manual step required** (Zenodo doesn't allow programmatic bulk download without API key):\n",
    "\n",
    "### For Bodenmiller Example Dataset\n",
    "1. Visit: https://zenodo.org/records/5949116\n",
    "2. Download all `.txt` files (IMC raw data)\n",
    "3. Place in: `benchmarks/data/bodenmiller_example/`\n",
    "\n",
    "### For High-res Kidney Dataset\n",
    "1. Visit: https://doi.org/10.5281/zenodo.17077712\n",
    "2. Download kidney tissue samples (`.txt` or `.mcd` format)\n",
    "3. Place in: `benchmarks/data/highres_kidney_2025/`\n",
    "\n",
    "**Alternative**: Use download script:\n",
    "```bash\n",
    "cd ../../\n",
    "./benchmarks/scripts/download_datasets.sh bodenmiller_example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imc_files(directory: Path) -> List[Path]:\n",
    "    \"\"\"Find all IMC .txt files in directory.\"\"\"\n",
    "    return sorted(directory.glob('**/*.txt'))\n",
    "\n",
    "def load_imc_txt(file_path: Path) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Load IMC .txt file and extract channel names.\n",
    "    \n",
    "    Returns:\n",
    "        data: (height, width, channels) array\n",
    "        channel_names: List of channel/marker names\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # First line contains channel names\n",
    "        first_line = f.readline().strip()\n",
    "        if first_line.startswith('Start'):\n",
    "            # Skip header lines until we find channel names\n",
    "            for line in f:\n",
    "                if not line.startswith('End'):\n",
    "                    channel_names = line.strip().split('\\t')\n",
    "                    break\n",
    "        else:\n",
    "            channel_names = first_line.split('\\t')\n",
    "    \n",
    "    # Load data (skip header)\n",
    "    data = np.loadtxt(file_path, skiprows=len(channel_names) + 2)\n",
    "    \n",
    "    return data, channel_names\n",
    "\n",
    "def compute_file_checksum(file_path: Path) -> str:\n",
    "    \"\"\"Compute MD5 checksum for file integrity verification.\"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b''):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "# Find all IMC files\n",
    "imc_files = find_imc_files(dataset_path)\n",
    "print(f\"Found {len(imc_files)} IMC .txt files\")\n",
    "\n",
    "if len(imc_files) == 0:\n",
    "    print(\"\\n⚠️  No .txt files found. Please download dataset first (see Step 1).\")\n",
    "else:\n",
    "    print(\"\\nFirst 5 files:\")\n",
    "    for f in imc_files[:5]:\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate files and collect metadata\n",
    "file_metadata = []\n",
    "\n",
    "for imc_file in imc_files:\n",
    "    try:\n",
    "        data, channels = load_imc_txt(imc_file)\n",
    "        checksum = compute_file_checksum(imc_file)\n",
    "        \n",
    "        metadata = {\n",
    "            'filename': imc_file.name,\n",
    "            'roi_name': imc_file.stem,  # Filename without extension\n",
    "            'file_size_mb': imc_file.stat().st_size / (1024 * 1024),\n",
    "            'n_channels': len(channels),\n",
    "            'image_shape': data.shape if len(data.shape) == 2 else (data.shape[0], data.shape[1]),\n",
    "            'n_pixels': data.size,\n",
    "            'checksum': checksum,\n",
    "            'channels': channels\n",
    "        }\n",
    "        file_metadata.append(metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {imc_file.name}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "metadata_df = pd.DataFrame([{k: v for k, v in m.items() if k != 'channels'} \n",
    "                            for m in file_metadata])\n",
    "\n",
    "print(f\"\\n✅ Successfully loaded {len(metadata_df)} files\")\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(metadata_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check channel consistency\n",
    "all_channels = [set(m['channels']) for m in file_metadata]\n",
    "common_channels = set.intersection(*all_channels) if all_channels else set()\n",
    "unique_channels = set.union(*all_channels) if all_channels else set()\n",
    "\n",
    "print(f\"Channel Analysis:\")\n",
    "print(f\"  Common to all ROIs: {len(common_channels)} channels\")\n",
    "print(f\"  Unique across all ROIs: {len(unique_channels)} channels\")\n",
    "\n",
    "if len(common_channels) < len(unique_channels):\n",
    "    print(\"\\n⚠️  Warning: Not all ROIs have same channel set\")\n",
    "    print(\"  Missing channels per ROI:\")\n",
    "    for m in file_metadata:\n",
    "        missing = unique_channels - set(m['channels'])\n",
    "        if missing:\n",
    "            print(f\"    {m['roi_name']}: {missing}\")\n",
    "else:\n",
    "    print(\"\\n✅ All ROIs have consistent channel set\")\n",
    "\n",
    "print(f\"\\nChannel List:\")\n",
    "for i, channel in enumerate(sorted(common_channels), 1):\n",
    "    print(f\"  {i:2d}. {channel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Metadata CSV\n",
    "\n",
    "Create metadata mapping ROI names to experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract experimental metadata from filenames if possible\n",
    "# Common IMC naming: <Study>_<Sample>_<ROI>_<Acquisition>.txt\n",
    "\n",
    "def parse_roi_name(filename: str) -> Dict[str, str]:\n",
    "    \"\"\"Attempt to parse experimental info from filename.\n",
    "    \n",
    "    This is dataset-specific and may need adjustment.\n",
    "    \"\"\"\n",
    "    # Remove extension\n",
    "    roi_name = Path(filename).stem\n",
    "    \n",
    "    # Example: IMC_241218_Alun_ROI_D7_M2_03_26\n",
    "    parts = roi_name.split('_')\n",
    "    \n",
    "    # Generic fallback\n",
    "    return {\n",
    "        'roi_name': roi_name,\n",
    "        'condition': 'unknown',\n",
    "        'timepoint': 'unknown',\n",
    "        'sample_id': 'unknown',\n",
    "        'region': 'unknown'\n",
    "    }\n",
    "\n",
    "# Create metadata dataframe\n",
    "roi_metadata_list = [parse_roi_name(m['filename']) for m in file_metadata]\n",
    "roi_metadata_df = pd.DataFrame(roi_metadata_list)\n",
    "\n",
    "print(\"Parsed ROI Metadata:\")\n",
    "print(roi_metadata_df.head(10))\n",
    "\n",
    "print(\"\\n⚠️  Review parsed metadata and manually correct if needed\")\n",
    "print(\"   Save corrected metadata to: benchmarks/data/<dataset>/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata template\n",
    "metadata_output = dataset_path / 'metadata_parsed.csv'\n",
    "roi_metadata_df.to_csv(metadata_output, index=False)\n",
    "print(f\"✅ Saved metadata to: {metadata_output}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review metadata_parsed.csv\")\n",
    "print(\"2. Manually correct condition/timepoint/sample_id columns\")\n",
    "print(\"3. Rename to metadata.csv when complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dataset summary\n",
    "dataset_summary = {\n",
    "    'dataset_name': DATASET_NAME,\n",
    "    'n_rois': len(file_metadata),\n",
    "    'n_channels': len(common_channels),\n",
    "    'channel_names': sorted(list(common_channels)),\n",
    "    'total_size_mb': sum(m['file_size_mb'] for m in file_metadata),\n",
    "    'image_dimensions': {\n",
    "        'min_width': int(metadata_df['image_shape'].apply(lambda x: x[0]).min()),\n",
    "        'max_width': int(metadata_df['image_shape'].apply(lambda x: x[0]).max()),\n",
    "        'min_height': int(metadata_df['image_shape'].apply(lambda x: x[1]).min()),\n",
    "        'max_height': int(metadata_df['image_shape'].apply(lambda x: x[1]).max()),\n",
    "    },\n",
    "    'total_pixels': int(metadata_df['n_pixels'].sum()),\n",
    "    'file_checksums': {m['filename']: m['checksum'] for m in file_metadata}\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_output = dataset_path / 'dataset_summary.json'\n",
    "with open(summary_output, 'w') as f:\n",
    "    json.dump(dataset_summary, f, indent=2)\n",
    "\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"  ROIs: {dataset_summary['n_rois']}\")\n",
    "print(f\"  Channels: {dataset_summary['n_channels']}\")\n",
    "print(f\"  Total size: {dataset_summary['total_size_mb']:.1f} MB\")\n",
    "print(f\"  Image dimensions: {dataset_summary['image_dimensions']}\")\n",
    "print(f\"\\n✅ Saved summary to: {summary_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare for Both Pipelines\n",
    "\n",
    "### Steinbock Format\n",
    "Steinbock expects:\n",
    "- `.txt` files in `img/` directory (already have this)\n",
    "- Optional `panel.csv` with channel metadata\n",
    "- Run via Docker wrapper script\n",
    "\n",
    "### Our Pipeline Format\n",
    "Our pipeline expects:\n",
    "- `.txt` files (same format)\n",
    "- `metadata.csv` with ROI annotations\n",
    "- `config.json` with analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Steinbock panel.csv\n",
    "panel_data = []\n",
    "for i, channel in enumerate(sorted(common_channels), 1):\n",
    "    # Attempt to parse metal and target from channel name\n",
    "    # Format often: <Metal><Mass>Di_<Target> or <Target>_<Metal><Mass>\n",
    "    if 'Di' in channel:\n",
    "        parts = channel.split('Di')\n",
    "        metal = parts[0] + 'Di'\n",
    "        target = parts[1].strip('_')\n",
    "    else:\n",
    "        metal = f\"Channel{i}\"\n",
    "        target = channel\n",
    "    \n",
    "    panel_data.append({\n",
    "        'channel': i,\n",
    "        'name': channel,\n",
    "        'metal': metal,\n",
    "        'target': target,\n",
    "        'keep': 1  # Keep all channels initially\n",
    "    })\n",
    "\n",
    "panel_df = pd.DataFrame(panel_data)\n",
    "panel_output = dataset_path / 'panel.csv'\n",
    "panel_df.to_csv(panel_output, index=False)\n",
    "\n",
    "print(f\"✅ Created Steinbock panel.csv: {panel_output}\")\n",
    "print(panel_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark config for our pipeline\n",
    "benchmark_config = {\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"data_directory\": str(dataset_path),\n",
    "    \"output_directory\": str(benchmark_dir / 'our_outputs' / DATASET_NAME),\n",
    "    \n",
    "    \"preprocessing\": {\n",
    "        \"arcsinh_cofactor\": 5,\n",
    "        \"background_subtraction\": False\n",
    "    },\n",
    "    \n",
    "    \"segmentation\": {\n",
    "        \"method\": \"slic\",\n",
    "        \"scales_um\": [10, 20, 40],\n",
    "        \"dna_channels\": [\"Ir191Di\", \"Ir193Di\"],  # Update based on actual panel\n",
    "        \"compactness\": 0.1,\n",
    "        \"sigma\": 1.0\n",
    "    },\n",
    "    \n",
    "    \"spatial_analysis\": {\n",
    "        \"k_neighbors\": 10,\n",
    "        \"n_permutations\": 500,\n",
    "        \"alpha\": 0.05\n",
    "    },\n",
    "    \n",
    "    \"clustering\": {\n",
    "        \"method\": \"kmeans\",\n",
    "        \"n_clusters_range\": [5, 20],\n",
    "        \"optimize\": True\n",
    "    },\n",
    "    \n",
    "    \"output_formats\": {\n",
    "        \"roi_results\": \"hdf5\",\n",
    "        \"summary\": \"json\",\n",
    "        \"spatial_graphs\": \"parquet\"\n",
    "    }\n",
    "}\n",
    "\n",
    "config_output = benchmark_dir / 'configs' / f'{DATASET_NAME}_config.json'\n",
    "config_output.parent.mkdir(exist_ok=True)\n",
    "with open(config_output, 'w') as f:\n",
    "    json.dump(benchmark_config, f, indent=2)\n",
    "\n",
    "print(f\"✅ Created benchmark config: {config_output}\")\n",
    "print(\"\\n⚠️  Review and update:\")\n",
    "print(\"   - DNA channels (based on actual panel)\")\n",
    "print(\"   - Cell type gating thresholds (if needed)\")\n",
    "print(\"   - Output paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset: {DATASET_NAME}\")\n",
    "print(f\"  Location: {dataset_path}\")\n",
    "print(f\"  ROIs: {len(file_metadata)}\")\n",
    "print(f\"  Channels: {len(common_channels)}\")\n",
    "print(f\"  Total size: {dataset_summary['total_size_mb']:.1f} MB\")\n",
    "\n",
    "print(\"\\nCreated Files:\")\n",
    "print(f\"  ✅ {metadata_output.name} - ROI metadata (review and rename)\")\n",
    "print(f\"  ✅ {summary_output.name} - Dataset summary with checksums\")\n",
    "print(f\"  ✅ {panel_output.name} - Steinbock panel file\")\n",
    "print(f\"  ✅ {config_output.name} - Our pipeline config\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review and correct metadata_parsed.csv → rename to metadata.csv\")\n",
    "print(\"  2. Update benchmark config (DNA channels, parameters)\")\n",
    "print(\"  3. Run Steinbock pipeline:\")\n",
    "print(f\"     cd {project_root}\")\n",
    "print(f\"     ./benchmarks/scripts/run_steinbock_docker.sh {dataset_path}\")\n",
    "print(\"  4. Run our pipeline:\")\n",
    "print(f\"     python run_analysis.py --config {config_output}\")\n",
    "print(\"  5. Proceed to comparison notebook: 04_quantitative_comparison.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
